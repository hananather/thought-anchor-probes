Metadata-Version: 2.4
Name: thought-anchor-probes
Version: 0.1.0
Summary: Sentence-level probes for Thought Anchors counterfactual importance
Author: MATS
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: huggingface-hub>=0.30.0
Requires-Dist: numpy>=1.26.0
Requires-Dist: pandas>=2.2.0
Requires-Dist: pyarrow>=15.0.0
Requires-Dist: pydantic>=2.8.0
Requires-Dist: pyyaml>=6.0.2
Requires-Dist: scikit-learn>=1.5.0
Requires-Dist: scipy>=1.13.0
Requires-Dist: torch>=2.2.0
Requires-Dist: tqdm>=4.66.0
Requires-Dist: transformers>=4.44.0
Provides-Extra: dev
Requires-Dist: pytest>=8.2.0; extra == "dev"
Requires-Dist: ruff>=0.6.0; extra == "dev"
Provides-Extra: verify
Requires-Dist: sentence-transformers>=3.0.1; extra == "verify"

# Thought Anchor Probes

Train sentence probes that predict counterfactual sentence importance from model activations.

## What You'll Do
- Load the ARENA Thought Anchors rollout dataset from Hugging Face.
- Build sentence labels from `counterfactual_importance_accuracy`.
- Extract one-layer sentence embeddings with mean pooling (default) or token-level ragged spans.
- Optionally extract sentence-level vertical attention scores with depth control (`off` / `light` / `full`).
- Train position, text-only, activation-only, activation+position, and vertical-attention baselines.
- Train token-aware `attention_probe` and `multimax_probe` when `activations.pooling: tokens`.
- Simulate cascade/deferral thresholds to plan efficient future labeling.
- Report top-k recall and precision-recall AUC.

## Why This Matters
This setup gives a cheap first test for Thought Anchor signal.
But sparse token-local evidence can be diluted by sentence-level averaging on long contexts.
Token-aware aggregation lets probes focus on the few relevant tokens instead of washing them out.

## Multi-token Probe Motivation
- Mean pooling can erase sparse signal: if only a few tokens carry anchor evidence, averaging over many neutral tokens shrinks the signal.
- Attention probes score transformed token activations per head and compute a weighted sum of per-token values.
- MultiMax replaces softmax averaging with per-head hard max over tokens, reducing long-context dilution.
- This PR-level design follows `Building Production-Ready Probes For Gemini` and Neel Nanda's repeated warning that tokenization-level confounds make fragile single-token interpretations easy to overread.
- The receiver-head/vertical-attention framing is aligned with ARENA-style interpretability workflows and the Thought Anchors analysis.
- References:
  - Kram√°r et al. (2026), *Building Production-Ready Probes For Gemini*: https://arxiv.org/abs/2601.11516
  - Neel Nanda (2024), tokenization confound cautionary example: https://www.neelnanda.io/mechanistic-interpretability/emergent-pos
  - Neel Nanda interview (2025), probe-utility caution in deployment contexts: https://80000hours.org/podcast/episodes/neel-nanda-mechanistic-interpretability/
  - ARENA Chapter 1 (transformer interpretability tutorials): https://github.com/callummcdougall/ARENA_3.0/tree/main/chapter1_transformer_interp
  - *Thought Anchors: Which LLM Reasoning Steps Matter?* (receiver heads and vertical attention): https://openreview.net/pdf/e2eb4189bc2250be1718a88fcb63c2423b280109.pdf

## Technical Requirements
- Python 3.10 or newer.
- `huggingface-hub` for dataset file listing and downloads.
- `transformers` and `torch` for model activations.
- `scikit-learn` for probe training.
- `pandas` and `pyarrow` for metadata tables.

## Quick Start
1. Create a virtual environment and install dependencies.
```bash
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]"
```
2. Run the full pilot + full experiment workflow.
```bash
python scripts/run_experiments.py \
  --config configs/experiment.yaml \
  --pilot-config configs/experiment_pilot.yaml \
  --full-config configs/experiment_full.yaml \
  --problem-id 330 \
  --readme-path README.md
```
3. Inspect per-stage artifacts.
```bash
ls artifacts/runs/pilot
ls artifacts/runs/full
```

## Optional Checks
- Run span integrity checks on one problem.
```bash
python scripts/check_spans.py --config configs/experiment.yaml --problem-id 330 --sample-size 20
```
- Recompute counterfactual labels for one problem.
```bash
pip install -e ".[verify]"
python scripts/verify_problem_labels.py --config configs/experiment.yaml --problem-id 330 --counterfactual
```
- Estimate disk usage for extracted activations (including ragged token mode).
```bash
python scripts/estimate_storage.py \
  --shape-json artifacts/runs/pilot/sentence_embeddings_shape.json \
  --metadata-parquet artifacts/runs/pilot/sentence_metadata.parquet
```

## MacBook Tips
- Start with `num_problems: 25` in `configs/experiment.yaml`.
- Keep `layer_mode: mid` and `pooling: mean` for low-memory runs.
- Use `device: auto` to select MPS when available.
- Avoid loading full rollouts except one verification problem.

## Output Files
- `artifacts/runs/pilot/sentence_embeddings.dat`
- `artifacts/runs/pilot/token_embeddings.dat` (when `activations.pooling: tokens`)
- `artifacts/runs/pilot/metrics_seed_*.json`
- `artifacts/runs/pilot/aggregate_metrics.json`
- `artifacts/runs/full/sentence_embeddings.dat`
- `artifacts/runs/full/token_embeddings.dat` (when `activations.pooling: tokens`)
- `artifacts/runs/full/metrics_seed_*.json`
- `artifacts/runs/full/aggregate_metrics.json`

## Cache Safety Invariants
- `scripts/train_probes.py` validates extraction-time provenance before fitting probes.
- If you change label or activation config (for example `labels.anchor_percentile`, `labels.drop_last_chunk`, or `activations.model_name_or_path`), re-run `scripts/extract_embeddings.py`.
- Training now fails fast when split problem IDs are missing from extracted metadata.
- `scripts/extract_embeddings.py --reuse-cache` skips extraction only when cache provenance matches the active config.
- Implementation details for maintainers: `docs/cache_provenance_guardrail.md`.

## Vertical Attention Baseline
- Vertical score definition: aggregate how strongly later tokens attend to each sentence span.
- `depth_control: true` normalizes by the number of later queries so early sentences are not over-favored just because they have more future tokens.
- Modes:
  - `light`: attention from only the final `N` reasoning tokens.
  - `full`: full sentence-to-sentence attention when `seq_len <= full_max_seq_len`; otherwise light fallback.
- Config snippet:
```yaml
activations:
  vertical_attention:
    mode: light
    depth_control: true
    light_last_n_tokens: 4
    full_max_seq_len: 1024
```
- When vertical scores are extracted, training adds:
  - `vertical_attention_baseline`
  - `vertical_attention_plus_position`

## Deferral Planning
- `scripts/plan_deferrals.py` tunes two thresholds on validation scores:
  - `score <= t_neg` => confident negative
  - `score >= t_pos` => confident positive
  - otherwise => defer
- Objectives:
  - `budget`: minimize accepted-set error subject to a max deferral budget.
  - `error`: minimize deferral while meeting a target accepted-set error.
- `scripts/train_probes.py` predictions now include `split` (`val` / `test`) so one parquet can drive tuning + projection directly.
- Example:
```bash
python scripts/plan_deferrals.py \
  --predictions artifacts/scaling/qwen_correct/predictions_seed_0.parquet \
  --score-column score_linear_probe \
  --objective budget \
  --deferral-budget 0.20 \
  --output artifacts/scaling/qwen_correct/deferral_plan_seed_0.json
```

## High-Confidence Scaling Matrix
- 4-setting configs are available under `configs/scaling_*.yaml`.
- Default matrix run executes the full four-setting replication grid:
```bash
python scripts/run_scaling_grid.py
```
- This default runs:
  - `configs/scaling_llama_correct.yaml`
  - `configs/scaling_llama_incorrect.yaml`
  - `configs/scaling_qwen_correct.yaml`
  - `configs/scaling_qwen_incorrect.yaml`
- To run only Qwen-14B settings, pass configs explicitly:
```bash
python scripts/run_scaling_grid.py \
  --configs \
  configs/scaling_qwen_correct.yaml \
  configs/scaling_qwen_incorrect.yaml
```
- RunPod memory: use at least 48 GB VRAM for stable 14B extraction runs.
- RunPod dependency contract (for reproducible VRAM/runtime behavior):
```bash
uv pip install -e . --system -c constraints/runpod.txt
python -c "import torch, transformers; print(torch.__version__, transformers.__version__)"
```
- Expected preflight versions: `2.9.1 4.57.3`.
- RunPod execution guide: `docs/runpod_scaling_runbook.md`.

## Repo Layout
- `src/ta_probe/data_loading.py`: dataset listing and fast metadata loading.
- `src/ta_probe/labels.py`: anchor labels from counterfactual scores.
- `src/ta_probe/spans.py`: chunking and sentence token boundaries.
- `src/ta_probe/activations.py`: one-layer hooks, pooled embeddings, and ragged token storage.
- `src/ta_probe/models.py`: sklearn baselines and mean-pooled probes.
- `src/ta_probe/token_probes.py`: torch attention and MultiMax token-level probes.
- `src/ta_probe/deferrals.py`: threshold tuning and projection for deferral simulation.
- `src/ta_probe/storage.py`: activation cache disk-usage estimation helpers.
- `src/ta_probe/train.py`: training, evaluation, and tripwire checks.
- `src/ta_probe/aggregate.py`: multi-seed metric aggregation.
- `src/ta_probe/readme_update.py`: deterministic README marker updates.
- `scripts/run_experiments.py`: end-to-end pilot + full orchestration.
- `scripts/run_scaling_grid.py`: four-setting scaling matrix orchestration.
- `scripts/plan_deferrals.py`: cascade/deferral simulation from saved prediction scores.
- `scripts/estimate_storage.py`: disk usage estimates from extracted metadata.
- `docs/why_mean_pooling_can_erase_sparse_signal.md`: mean-pooling dilution and MultiMax rationale.
- `tests/`: unit tests for spans, labels, and metrics.

## Experiment Results

<!-- EXPERIMENT_RESULTS_START -->
Last updated: 2026-02-23 01:24:26

### Objective
Run the full Thought Anchor probe plan with pilot and full stages.

### Environment
- Host: 5dcc8e944ec4
- Platform: Linux-6.8.0-60-generic-x86_64-with-glibc2.35
- Python: 3.11.10
- Dataset listing date context: February 22, 2026

### Commands Executed
- `python -m ruff check .` | exit=0 | 0.12s
- `python -m pytest -q` | exit=0 | 8.12s
- `python scripts/build_problem_index.py --config configs/experiment.yaml --refresh` | exit=0 | 0.59s
- `python scripts/verify_problem_labels.py --config configs/experiment.yaml --problem-id 330` | exit=0 | 7.48s
- `python scripts/check_spans.py --config configs/experiment.yaml --problem-id 330 --sample-size 20` | exit=0 | 7.07s
- `python scripts/build_problem_index.py --config configs/experiment_pilot.yaml --refresh` | exit=0 | 0.60s
- `python scripts/build_problem_index.py --config configs/experiment_full.yaml --refresh` | exit=0 | 0.60s
- `python scripts/extract_embeddings.py --config configs/experiment_pilot.yaml --skip-failed --failure-log /workspace/thought-anchor-probes/artifacts/runs/pilot/extraction_failures.json --reuse-cache` | exit=0 | 33.27s
- `python scripts/train_probes.py --config configs/experiment_pilot.yaml --seed 0 --run-name seed_0` | exit=0 | 10.95s
- `python scripts/train_probes.py --config configs/experiment_pilot.yaml --seed 1 --run-name seed_1` | exit=0 | 11.10s
- `python scripts/train_probes.py --config configs/experiment_pilot.yaml --seed 2 --run-name seed_2` | exit=0 | 11.31s
- `python scripts/aggregate_runs.py --run-root /workspace/thought-anchor-probes/artifacts/runs/pilot` | exit=0 | 0.67s
- `python scripts/extract_embeddings.py --config configs/experiment_full.yaml --skip-failed --failure-log /workspace/thought-anchor-probes/artifacts/runs/full/extraction_failures.json --reuse-cache` | exit=0 | 37.97s
- `python scripts/train_probes.py --config configs/experiment_full.yaml --seed 0 --run-name seed_0` | exit=0 | 25.87s
- `python scripts/train_probes.py --config configs/experiment_full.yaml --seed 1 --run-name seed_1` | exit=0 | 24.72s
- `python scripts/train_probes.py --config configs/experiment_full.yaml --seed 2 --run-name seed_2` | exit=0 | 25.08s
- `python scripts/aggregate_runs.py --run-root /workspace/thought-anchor-probes/artifacts/runs/full` | exit=0 | 0.63s

### Verification Gates
- Resampling avg diff: 0.008326
- Resampling pass: True
- Span pass rate: 1.000

### Dataset and Split Summary
- Pilot split sizes: train=3, val=1, test=1
- Full split sizes: train=14, val=3, test=3

### Pilot Per-Seed Test Metrics
| Run | Seed | Model | PR AUC | Spearman | Top-5 | Top-10 |
|---|---:|---|---:|---:|---:|---:|
| seed_0 | 0 | activations_plus_position | 0.1472 | 0.1386 | 0.0000 | 0.0000 |
| seed_0 | 0 | linear_probe | 0.1478 | 0.1380 | 0.0000 | 0.0000 |
| seed_0 | 0 | mlp_probe | 0.1565 | 0.2705 | 0.0000 | 0.0000 |
| seed_0 | 0 | position_baseline | 0.1415 | 0.4686 | 0.0000 | 0.1000 |
| seed_0 | 0 | text_only_baseline | 0.0955 | -0.0274 | 0.0000 | 0.0000 |
| seed_1 | 1 | activations_plus_position | 0.1472 | 0.1386 | 0.0000 | 0.0000 |
| seed_1 | 1 | linear_probe | 0.1478 | 0.1380 | 0.0000 | 0.0000 |
| seed_1 | 1 | mlp_probe | 0.0993 | 0.0944 | 0.0000 | 0.0000 |
| seed_1 | 1 | position_baseline | 0.1415 | 0.4686 | 0.0000 | 0.1000 |
| seed_1 | 1 | text_only_baseline | 0.0955 | -0.0274 | 0.0000 | 0.0000 |
| seed_2 | 2 | activations_plus_position | 0.1472 | 0.1386 | 0.0000 | 0.0000 |
| seed_2 | 2 | linear_probe | 0.1478 | 0.1380 | 0.0000 | 0.0000 |
| seed_2 | 2 | mlp_probe | 0.1866 | 0.2109 | 0.2000 | 0.2000 |
| seed_2 | 2 | position_baseline | 0.1415 | 0.4686 | 0.0000 | 0.1000 |
| seed_2 | 2 | text_only_baseline | 0.0955 | -0.0274 | 0.0000 | 0.0000 |

### Pilot Mean and Std Across Seeds
| Model | PR AUC mean | PR AUC std | Spearman mean | Spearman std | Top-5 mean | Top-5 std | Top-10 mean | Top-10 std |
|---|---:|---:|---:|---:|---:|---:|---:|---:|
| linear_probe | 0.1478 | 0.0000 | 0.1380 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |
| mlp_probe | 0.1475 | 0.0362 | 0.1919 | 0.0731 | 0.0667 | 0.0943 | 0.0667 | 0.0943 |
| activations_plus_position | 0.1472 | 0.0000 | 0.1386 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |
| position_baseline | 0.1415 | 0.0000 | 0.4686 | 0.0000 | 0.0000 | 0.0000 | 0.1000 | 0.0000 |
| text_only_baseline | 0.0955 | 0.0000 | -0.0274 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |

- Pilot best model by mean PR AUC: `linear_probe`

### Pilot Tripwire Outcomes
| Run | Random-label near chance | Random-label PR AUC | Prevalence | Overfit can memorize | Overfit PR AUC |
|---|---|---:|---:|---|---:|
| seed_0 | True | 0.0875 | 0.1003 | False | 0.7571 |
| seed_1 | True | 0.0841 | 0.1003 | False | 0.7274 |
| seed_2 | True | 0.0981 | 0.1003 | False | 0.8439 |

### Pilot Bootstrap CI Summary
| Comparison | Point delta mean | Point delta std | Bootstrap delta mean | Bootstrap delta std | Seeds CI excludes 0 | Seeds |
|---|---:|---:|---:|---:|---:|---:|
| score_activations_plus_position_minus_score_position_baseline | 0.0057 | 0.0000 | 0.0057 | 0.0000 | 3 | 3 |
| score_activations_plus_position_minus_score_text_only_baseline | 0.0518 | 0.0000 | 0.0518 | 0.0000 | 3 | 3 |

### Pilot Extraction Failures
- No extraction failures were logged.

### Full Per-Seed Test Metrics
| Run | Seed | Model | PR AUC | Spearman | Top-5 | Top-10 |
|---|---:|---|---:|---:|---:|---:|
| seed_0 | 0 | activations_plus_position | 0.1679 | 0.2286 | 0.0667 | 0.1333 |
| seed_0 | 0 | linear_probe | 0.1670 | 0.2261 | 0.0667 | 0.1667 |
| seed_0 | 0 | mlp_probe | 0.1523 | 0.2560 | 0.0000 | 0.0667 |
| seed_0 | 0 | position_baseline | 0.1625 | 0.5125 | 0.0667 | 0.1000 |
| seed_0 | 0 | text_only_baseline | 0.1032 | -0.0135 | 0.0667 | 0.0667 |
| seed_1 | 1 | activations_plus_position | 0.1679 | 0.2286 | 0.0667 | 0.1333 |
| seed_1 | 1 | linear_probe | 0.1670 | 0.2261 | 0.0667 | 0.1667 |
| seed_1 | 1 | mlp_probe | 0.1354 | 0.1917 | 0.0667 | 0.0667 |
| seed_1 | 1 | position_baseline | 0.1625 | 0.5125 | 0.0667 | 0.1000 |
| seed_1 | 1 | text_only_baseline | 0.1032 | -0.0135 | 0.0667 | 0.0667 |
| seed_2 | 2 | activations_plus_position | 0.1679 | 0.2286 | 0.0667 | 0.1333 |
| seed_2 | 2 | linear_probe | 0.1670 | 0.2261 | 0.0667 | 0.1667 |
| seed_2 | 2 | mlp_probe | 0.1911 | 0.3234 | 0.0667 | 0.1000 |
| seed_2 | 2 | position_baseline | 0.1625 | 0.5125 | 0.0667 | 0.1000 |
| seed_2 | 2 | text_only_baseline | 0.1032 | -0.0135 | 0.0667 | 0.0667 |

### Full Mean and Std Across Seeds
| Model | PR AUC mean | PR AUC std | Spearman mean | Spearman std | Top-5 mean | Top-5 std | Top-10 mean | Top-10 std |
|---|---:|---:|---:|---:|---:|---:|---:|---:|
| activations_plus_position | 0.1679 | 0.0000 | 0.2286 | 0.0000 | 0.0667 | 0.0000 | 0.1333 | 0.0000 |
| linear_probe | 0.1670 | 0.0000 | 0.2261 | 0.0000 | 0.0667 | 0.0000 | 0.1667 | 0.0000 |
| position_baseline | 0.1625 | 0.0000 | 0.5125 | 0.0000 | 0.0667 | 0.0000 | 0.1000 | 0.0000 |
| mlp_probe | 0.1596 | 0.0233 | 0.2570 | 0.0538 | 0.0444 | 0.0314 | 0.0778 | 0.0157 |
| text_only_baseline | 0.1032 | 0.0000 | -0.0135 | 0.0000 | 0.0667 | 0.0000 | 0.0667 | 0.0000 |

- Full best model by mean PR AUC: `activations_plus_position`

### Full Tripwire Outcomes
| Run | Random-label near chance | Random-label PR AUC | Prevalence | Overfit can memorize | Overfit PR AUC |
|---|---|---:|---:|---|---:|
| seed_0 | True | 0.0767 | 0.1009 | True | 0.9029 |
| seed_1 | True | 0.0788 | 0.1009 | True | 0.9553 |
| seed_2 | True | 0.1016 | 0.1009 | True | 0.9495 |

### Full Bootstrap CI Summary
| Comparison | Point delta mean | Point delta std | Bootstrap delta mean | Bootstrap delta std | Seeds CI excludes 0 | Seeds |
|---|---:|---:|---:|---:|---:|---:|
| score_activations_plus_position_minus_score_position_baseline | 0.0055 | 0.0000 | 0.0074 | 0.0005 | 0 | 3 |
| score_activations_plus_position_minus_score_text_only_baseline | 0.0647 | 0.0000 | 0.0635 | 0.0004 | 3 | 3 |

### Full Extraction Failures
- No extraction failures were logged.

### Methodology Fidelity
- Planned and executed: resampling verification, span checks, pilot gate, full run, and three seeds.
- Planned and executed: position baseline, text-only baseline, linear probe, MLP probe, and activations+position probe.
- Planned and executed: problem-level train, validation, and test splits.
- Recommended for small `N_problems`: LOPO-CV with per-problem paired deltas (see `docs/lopo_cv.md`).
- Beyond-position evaluation (residualization) is available via `docs/residualization.md`.

### Deviations
#### Minor Changes
- Verification uses fixed problem ID `330` for determinism.
- Full config uses `num_problems: 9999` to include all available IDs automatically.

#### Major Methodology Changes
1. Three-seed training and aggregation added.
2. Skip-and-log extraction policy added.
3. Pilot gate before full run added.
<!-- EXPERIMENT_RESULTS_END -->

## Scaling Grid Results

Four-setting replication matrix run on an NVIDIA L40S (44.4 GB VRAM), 5 seeds per setting, 20 shared problems (train=14, val=3, test=3).

### Summary

| Setting | Best Model | Mean PR-AUC | Spearman |
|---|---|---:|---:|
| Llama-8B correct | mlp_probe | 0.1682 | 0.2545 |
| Llama-8B incorrect | position_baseline | 0.1892 | 0.2155 |
| Qwen-14B correct | position_baseline | 0.1801 | 0.4799 |
| Qwen-14B incorrect | mlp_probe | 0.1411 | 0.1520 |

### Per-Setting Model Comparison (Test Set, 5-seed mean)

#### Llama-8B Correct
| Model | PR-AUC | std | Spearman |
|---|---:|---:|---:|
| mlp_probe | 0.1682 | 0.0202 | 0.2545 |
| activations_plus_position | 0.1679 | 0.0000 | 0.2250 |
| linear_probe | 0.1664 | 0.0000 | 0.2157 |
| position_baseline | 0.1625 | 0.0000 | 0.5125 |
| text_only_baseline | 0.1032 | 0.0000 | -0.0135 |

#### Llama-8B Incorrect
| Model | PR-AUC | std | Spearman |
|---|---:|---:|---:|
| position_baseline | 0.1892 | 0.0000 | 0.2155 |
| activations_plus_position | 0.1319 | 0.0000 | -0.0171 |
| linear_probe | 0.1272 | 0.0000 | -0.0330 |
| mlp_probe | 0.1265 | 0.0122 | 0.0573 |
| text_only_baseline | 0.0998 | 0.0000 | -0.0367 |

#### Qwen-14B Correct
| Model | PR-AUC | std | Spearman |
|---|---:|---:|---:|
| position_baseline | 0.1801 | 0.0000 | 0.4799 |
| mlp_probe | 0.1436 | 0.0272 | 0.2601 |
| activations_plus_position | 0.1190 | 0.0000 | 0.1773 |
| linear_probe | 0.1187 | 0.0000 | 0.1703 |
| text_only_baseline | 0.0961 | 0.0000 | 0.0876 |

#### Qwen-14B Incorrect
| Model | PR-AUC | std | Spearman |
|---|---:|---:|---:|
| mlp_probe | 0.1411 | 0.0083 | 0.1520 |
| linear_probe | 0.1249 | 0.0000 | 0.0591 |
| activations_plus_position | 0.1249 | 0.0000 | 0.0594 |
| text_only_baseline | 0.1186 | 0.0000 | 0.0960 |
| position_baseline | 0.1107 | 0.0000 | 0.1379 |

### Bootstrap CI: activations_plus_position vs Baselines

| Setting | vs position_baseline (delta) | CI excludes 0 | vs text_only_baseline (delta) | CI excludes 0 |
|---|---:|---|---:|---|
| Llama-8B correct | +0.0070 | 0/5 seeds | +0.0628 | 5/5 seeds |
| Llama-8B incorrect | -0.0560 | 5/5 seeds | +0.0289 | 0/5 seeds |
| Qwen-14B correct | -0.0768 | 0/5 seeds | +0.0240 | 0/5 seeds |
| Qwen-14B incorrect | +0.0116 | 5/5 seeds | +0.0133 | 0/5 seeds |
